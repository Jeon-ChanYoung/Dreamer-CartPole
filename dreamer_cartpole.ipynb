{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb9c9718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gymnasium: 1.2.2\n",
      "cv2: 4.12.0\n",
      "numpy: 2.0.2\n",
      "torch: 2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import kl, Normal, Independent, OneHotCategorical\n",
    "from torch.nn.init import kaiming_uniform_, constant_\n",
    "\n",
    "print(\"gymnasium:\", gym.__version__)\n",
    "print(\"cv2:\", cv2.__version__)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"torch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02cccf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, width, height):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255,\n",
    "            shape=(self.height, self.width, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.env.render()\n",
    "        observation = cv2.resize(observation, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return observation\n",
    "\n",
    "class ChannelFirstEnv(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_space = self.observation_space\n",
    "        obs_shape = obs_space.shape[-1:] + obs_space.shape[:2]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=obs_shape, dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    # permute [H, W, C] array to [C, H, W] tensor\n",
    "    def observation(self, observation):\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        return observation\n",
    "\n",
    "class FrameSkipWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        return next_state, total_reward, terminated, truncated, info\n",
    "\n",
    "class PixelNormalizationWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return self._pixel_normalization(next_state), reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self):\n",
    "        state, info = self.env.reset()\n",
    "        return self._pixel_normalization(state), info\n",
    "\n",
    "    def _pixel_normalization(self, state):\n",
    "        return state / 255.0 - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c63642c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dreamer:\n",
    "    def __init__(self, model_path=\"\"):\n",
    "        # model\n",
    "        self.encoder = Encoder().to(device)\n",
    "        self.decoder = Decoder().to(device)\n",
    "        self.rssm = RSSM().to(device)\n",
    "        self.reward_predictor = RewardPredictor().to(device)\n",
    "        self.actor = Actor().to(device)\n",
    "        self.critic = Critic().to(device)\n",
    "\n",
    "        self.memory = ReplayBuffer()\n",
    "\n",
    "        # optimizer\n",
    "        self.world_model_params = (\n",
    "              list(self.encoder.parameters())\n",
    "            + list(self.decoder.parameters())\n",
    "            + list(self.rssm.parameters())\n",
    "            + list(self.reward_predictor.parameters())\n",
    "        )\n",
    "\n",
    "        self.world_model_optimizer = optim.Adam(self.world_model_params, lr=world_model_lr)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.reconstruction_loss = 0\n",
    "        self.reward_loss = 0\n",
    "        self.kl_loss = 0\n",
    "        self.actor_loss = 0\n",
    "        self.critic_loss = 0\n",
    "        self.score = 0\n",
    "\n",
    "        if model_path:\n",
    "            print(f\"Loading checkpoint from: {model_path}\")\n",
    "            self.load_checkpoint(model_path)\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "        self.encoder.load_state_dict(checkpoint['encoder'])\n",
    "        self.decoder.load_state_dict(checkpoint['decoder'])\n",
    "        self.rssm.load_state_dict(checkpoint['rssm'])\n",
    "        self.reward_predictor.load_state_dict(checkpoint['reward_predictor'])\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "        self.critic.load_state_dict(checkpoint['critic'])\n",
    "\n",
    "        self.world_model_optimizer.load_state_dict(checkpoint['world_model_optimizer'])\n",
    "        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n",
    "        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])\n",
    "        print(\"Checkpoint loaded successfully.\")\n",
    "\n",
    "    def train(self, env):\n",
    "        print(\"Collecting experiences...\")\n",
    "        self.collect_episodes(env, 20) # 초기 데이터 수집\n",
    "        print(f\"Collected experiences : {len(self.memory)}\")\n",
    "\n",
    "        for episode in range(1, episodes + 1):\n",
    "            for _ in range(world_model_update_steps):\n",
    "                experiences = self.memory.sample(batch_size, batch_length)\n",
    "                posteriors, deterministics = self.dynamic_learning(experiences)\n",
    "                self.behavior_learning(posteriors, deterministics)\n",
    "\n",
    "            self.collect_episodes(env, collect_episodes)\n",
    "            self.print_losses(episode) # 학습 손실 출력\n",
    "\n",
    "            if episode % 10 == 0:\n",
    "                torch.save({\n",
    "                    'encoder': self.encoder.state_dict(),\n",
    "                    'decoder': self.decoder.state_dict(),\n",
    "                    'rssm': self.rssm.state_dict(),\n",
    "                    'reward_predictor': self.reward_predictor.state_dict(),\n",
    "                    'actor': self.actor.state_dict(),\n",
    "                    'critic': self.critic.state_dict(),\n",
    "                    'world_model_optimizer': self.world_model_optimizer.state_dict(),\n",
    "                    'actor_optimizer': self.actor_optimizer.state_dict(),\n",
    "                    'critic_optimizer': self.critic_optimizer.state_dict(),\n",
    "                }, f'model_ep{episode}.pth')\n",
    "\n",
    "    def print_losses(self, episode):\n",
    "        print(f\"episode: {episode:4} | \"\n",
    "              f\"score: {int(self.score):3} | \"\n",
    "              f\"Recon: {self.reconstruction_loss.item():8.2f} | \"\n",
    "              f\"Reward: {self.reward_loss.item():6.4f} | \"\n",
    "              f\"KL: {self.kl_loss.item():6.4f} | \"\n",
    "              f\"Actor: {self.actor_loss.item():6.4f} | \"\n",
    "              f\"Critic: {self.critic_loss.item():6.4f}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def collect_episodes(self, env, episodes):\n",
    "        for episode in range(episodes):\n",
    "            posterior, deterministic = self.rssm.recurrent_model_input_init(1) # (1, 30), (1, 200)\n",
    "            action = torch.zeros(1, action_size).to(device)                    # (1, 2)\n",
    "\n",
    "            state, _ = env.reset() # (3, 64, 64)\n",
    "            embedded_state = self.encoder(torch.from_numpy(state).unsqueeze(0).float().to(device)) # (1, 1024)\n",
    "\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                deterministic = self.rssm.recurrent_model(posterior, action, deterministic)  # (1, 30), (1, 2), (1, 200) -> (1, 200)\n",
    "                _, posterior = self.rssm.representation_model(embedded_state, deterministic) # (1, 1024), (1, 200) -> (1, 30)\n",
    "                action = self.actor(posterior, deterministic).detach()                       # (1, 200), (1, 30) -> (1, 2)\n",
    "\n",
    "                buffer_action = action.cpu().numpy()\n",
    "                env_action = buffer_action.argmax()\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = env.step(env_action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                self.memory.add(state, buffer_action, reward, next_state, done)\n",
    "\n",
    "                embedded_state = self.encoder(torch.from_numpy(next_state).unsqueeze(0).float().to(device))\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            self.score = episode_reward\n",
    "\n",
    "    def dynamic_learning(self, experiences):\n",
    "        \"\"\"\n",
    "        [experiences]\n",
    "        states:      (50, 50, 3, 64, 64)\n",
    "        next_states: (50, 50, 3, 64, 64)\n",
    "        actions:     (50, 50, 2)\n",
    "        rewards:     (50, 50, 1)\n",
    "        dones:       (50, 50, 1)\n",
    "        \"\"\"\n",
    "        prior_means = []\n",
    "        prior_stds = []\n",
    "        posteriors = []\n",
    "        posterior_means = []\n",
    "        posterior_stds = []\n",
    "        deterministics = []\n",
    "\n",
    "        states, _, actions, _, _ = experiences\n",
    "        prior, deterministic = self.rssm.recurrent_model_input_init(batch_size) # (50, 30), (50, 200)\n",
    "        embedded_states = self.encoder(states) # (50, 50, 3, 64, 64) -> (50, 50, 1024)\n",
    "\n",
    "        for t in range(1, batch_length):\n",
    "            deterministic = self.rssm.recurrent_model(prior, actions[:, t - 1], deterministic) # (50, 30), (50, 2), (50, 200) -> (50, 200)\n",
    "            prior_dist, prior = self.rssm.transition_model(deterministic)\n",
    "            posterior_dist, posterior = self.rssm.representation_model(embedded_states[:, t], deterministic)\n",
    "\n",
    "            prior_means.append(prior_dist.mean)\n",
    "            prior_stds.append(prior_dist.scale)\n",
    "            posteriors.append(posterior)\n",
    "            posterior_means.append(posterior_dist.mean)\n",
    "            posterior_stds.append(posterior_dist.scale)\n",
    "            deterministics.append(deterministic)\n",
    "\n",
    "            prior = posterior\n",
    "\n",
    "        # size: (50, 49, 30)\n",
    "        prior_means = torch.stack(prior_means, dim=1)\n",
    "        prior_stds = torch.stack(prior_stds, dim=1)\n",
    "        posteriors = torch.stack(posteriors, dim=1)\n",
    "        posterior_means = torch.stack(posterior_means, dim=1)\n",
    "        posterior_stds = torch.stack(posterior_stds, dim=1)\n",
    "\n",
    "        # size: (50, 49, 200)\n",
    "        deterministics = torch.stack(deterministics, dim=1)\n",
    "\n",
    "        self.world_model_update(\n",
    "            experiences=experiences,\n",
    "            prior_means=prior_means,\n",
    "            prior_stds=prior_stds,\n",
    "            posteriors=posteriors,\n",
    "            posterior_means=posterior_means,\n",
    "            posterior_stds=posterior_stds,\n",
    "            deterministics=deterministics,\n",
    "        )\n",
    "        return posteriors.detach(), deterministics.detach()\n",
    "\n",
    "    def world_model_update(\n",
    "        self,\n",
    "        experiences,\n",
    "        prior_means,\n",
    "        prior_stds,\n",
    "        posteriors,\n",
    "        posterior_means,\n",
    "        posterior_stds,\n",
    "        deterministics,\n",
    "    ):\n",
    "        states, _, _, rewards, _ = experiences\n",
    "\n",
    "        # reconstruction loss\n",
    "        reconstruction_dist = self.decoder(posteriors, deterministics)\n",
    "        reconstruction_loss = reconstruction_dist.log_prob(states[:, 1:])\n",
    "\n",
    "        # reward loss\n",
    "        reward_dist = self.reward_predictor(posteriors, deterministics)\n",
    "        reward_loss = reward_dist.log_prob(rewards[:, 1:])\n",
    "\n",
    "        # kl divergence loss\n",
    "        prior_dist = create_normal_dist(prior_means, prior_stds, event_shape=1,)\n",
    "        posterior_dist = create_normal_dist(posterior_means, posterior_stds, event_shape=1,)\n",
    "\n",
    "        kl_loss = torch.mean(kl.kl_divergence(posterior_dist, prior_dist))\n",
    "        kl_loss = torch.max(torch.tensor(free_nats).to(device), kl_loss)\n",
    "\n",
    "        world_model_loss = (\n",
    "              kl_loss\n",
    "            - reconstruction_loss.mean()\n",
    "            - reward_loss.mean()\n",
    "        )\n",
    "\n",
    "        self.world_model_optimizer.zero_grad()\n",
    "        world_model_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.world_model_params, clip_grad, grad_norm_type)\n",
    "        self.world_model_optimizer.step()\n",
    "\n",
    "        self.reconstruction_loss = -reconstruction_loss.mean()\n",
    "        self.reward_loss = -reward_loss.mean()\n",
    "        self.kl_loss = kl_loss\n",
    "\n",
    "    def behavior_learning(self, posteriors, deterministics):\n",
    "        priors = posteriors.reshape(-1, stochastic_size) # (50, 49, 30) -> (2450, 30)\n",
    "        deterministics = deterministics.reshape(-1, deterministic_size) # (50, 49, 200) -> (2450, 200)\n",
    "\n",
    "        imagined_states = []\n",
    "        imagined_deterministics = []\n",
    "\n",
    "        for t in range(horizon_length):\n",
    "            action = self.actor(priors, deterministics) # (2450, 30), (2450, 200) -> (2450, 2)\n",
    "            deterministics = self.rssm.recurrent_model(priors, action, deterministics) # (2450, 30), (2450, 2), (2450, 200) -> (2450, 200)\n",
    "            _, priors = self.rssm.transition_model(deterministics) # (2450, 200) -> (2450, 30)\n",
    "\n",
    "            imagined_states.append(priors)\n",
    "            imagined_deterministics.append(deterministics)\n",
    "\n",
    "        imagined_states = torch.stack(imagined_states, dim=1)                 # (2450, 15, 30)\n",
    "        imagined_deterministics = torch.stack(imagined_deterministics, dim=1) # (2450, 15, 200)\n",
    "\n",
    "        self.agent_update(imagined_states, imagined_deterministics)\n",
    "\n",
    "    def agent_update(self, priors, deterministics):\n",
    "        predicted_rewards = self.reward_predictor(priors, deterministics).mean # (2450, 15, 1)\n",
    "        values = self.critic(priors, deterministics)                           # (2450, 15, 1)\n",
    "\n",
    "        lambda_returns = compute_lambda_returns(predicted_rewards, values)\n",
    "\n",
    "        # 1) Actor\n",
    "        actor_loss = -torch.mean(lambda_returns)\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.actor.parameters(), clip_grad, grad_norm_type)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 2) Critic\n",
    "        pred_values = self.critic(priors.detach()[:, :-1], deterministics.detach()[:, :-1])\n",
    "        target_values = lambda_returns.detach()\n",
    "        value_loss = F.mse_loss(pred_values, target_values)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic.parameters(), clip_grad, grad_norm_type)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        self.actor_loss = actor_loss\n",
    "        self.critic_loss = value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afb91ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self):\n",
    "        self.index = 0\n",
    "        self.capacity = capacity\n",
    "\n",
    "        self.states_buffer      = np.empty((self.capacity, *observation_shape), dtype=np.float32)\n",
    "        self.next_states_buffer = np.empty((self.capacity, *observation_shape), dtype=np.float32)\n",
    "        self.actions_buffer     = np.empty((self.capacity, action_size), dtype=np.float32)\n",
    "        self.rewards_buffer     = np.empty((self.capacity, 1), dtype=np.float32)\n",
    "        self.dones_buffer       = np.empty((self.capacity, 1), dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.index if self.index < self.capacity else self.capacity\n",
    "\n",
    "    def add(self, observation, action, reward, next_observation, done):\n",
    "        self.states_buffer[self.index]      = observation\n",
    "        self.actions_buffer[self.index]     = action\n",
    "        self.rewards_buffer[self.index]     = reward\n",
    "        self.next_states_buffer[self.index] = next_observation\n",
    "        self.dones_buffer[self.index]       = done\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, seq_len):\n",
    "        current_size = len(self)\n",
    "\n",
    "        if current_size < seq_len:\n",
    "            print(\"too small dataset\")\n",
    "            return\n",
    "\n",
    "        # 버퍼가 꽉 차면 아무데니 시작해도 유효함\n",
    "        # 아직 덜 찼으면 시퀀스가 범위를 넘지 않도록 시작점 제한\n",
    "        max_start_index = current_size if current_size == self.capacity else current_size - seq_len + 1\n",
    "        start_indices = np.random.randint(0, max_start_index, batch_size).reshape(-1, 1) # (50, 1)\n",
    "        offsets = np.arange(seq_len).reshape(1, -1)                                      # (1, 50)\n",
    "        indices = (start_indices + offsets) % self.capacity                              # (50, 50)\n",
    "\n",
    "        states      = torch.as_tensor(self.states_buffer[indices], device=device).float()\n",
    "        next_states = torch.as_tensor(self.next_states_buffer[indices], device=device).float()\n",
    "        actions     = torch.as_tensor(self.actions_buffer[indices], device=device)\n",
    "        rewards     = torch.as_tensor(self.rewards_buffer[indices], device=device)\n",
    "        dones       = torch.as_tensor(self.dones_buffer[indices], device=device)\n",
    "\n",
    "        \"\"\"\n",
    "        states:      (50, 50, 3, 64, 64)\n",
    "        next_states: (50, 50, 3, 64, 64)\n",
    "        actions:     (50, 50, 2)\n",
    "        rewards:     (50, 50, 1)\n",
    "        dones:       (50, 50, 1)\n",
    "        \"\"\"\n",
    "        return states, next_states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d7c3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Encoder]\n",
    "input  : (B, 3, 64, 64) or (B, T, 3, 64, 64)\n",
    "output : (B, 1024) or (B, T, 1024)\n",
    "\"\"\"\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2),    # (B, 3, 64, 64)  -> (B, 32, 31, 31)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2),   # (B, 32, 31, 31) -> (B, 64, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2),  # (B, 64, 14, 14) -> (B, 128, 6, 6)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2), # (B, 128, 6, 6)  -> (B, 256, 2, 2)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),              # (B, 256, 2, 2)  -> (B, 1024)\n",
    "        )\n",
    "        self.network.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 5: # (B, T, C, H, W)\n",
    "            B, T, C, H, W = x.shape\n",
    "            x = x.view(B*T, C, H, W)\n",
    "            x = self.network(x)\n",
    "            x = x.view(B, T, -1)\n",
    "        else:           # (B, C, H, W)\n",
    "            x = self.network(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5eef7ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Decoder]\n",
    "input  : posterior (B*T, 30), deterministic (B*T, 200)\n",
    "output : Normal dist (B, T, 3, 64, 64)\n",
    "\"\"\"\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(stochastic_size + deterministic_size, 1024), # (B*T, 230) -> (B*T, 1024)\n",
    "            nn.Unflatten(1, (1024, 1, 1)),       # (B*T, 1024)       -> (B*T, 1024, 1, 1)\n",
    "            nn.ConvTranspose2d(1024, 128, 5, 2), # (B*T, 1024, 1, 1) -> (B*T, 128, 5, 5)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 5, 2),   # (B*T, 128, 5, 5)  -> (B*T, 64, 13, 13)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 6, 2),    # (B*T, 64, 13, 13) -> (B*T, 32, 30, 30)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 6, 2),     # (B*T, 32, 30, 30) -> (B*T, 3, 64, 64)\n",
    "        )\n",
    "        self.network.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, posterior, deterministic):\n",
    "        x = temporal_forward(self.network, posterior, deterministic, output_shape=observation_shape)\n",
    "        dist = create_normal_dist(x, std=1, event_shape=len(observation_shape))\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41086a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSSM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.recurrent_model = RecurrentModel()\n",
    "        self.transition_model = TransitionModel()\n",
    "        self.representation_model = RepresentationModel()\n",
    "\n",
    "    def recurrent_model_input_init(self, batch_size):\n",
    "        return self.transition_model.input_init(batch_size), self.recurrent_model.input_init(batch_size)\n",
    "\n",
    "\"\"\"\n",
    "[RecurrentModel]\n",
    "input  : stochastic (B, 30), action (B, 2), deterministic (B, 200)\n",
    "output : deterministic (B, 200)\n",
    "\"\"\"\n",
    "class RecurrentModel(nn.Module):\n",
    "    def __init__(self, hidden_size=200):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(stochastic_size + action_size, self.hidden_size),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.recurrent = nn.GRUCell(self.hidden_size, deterministic_size)\n",
    "\n",
    "    def forward(self, stochastic, action, deterministic):\n",
    "        x = torch.cat((stochastic, action), 1)\n",
    "        x = self.network(x)\n",
    "        x = self.recurrent(x, deterministic)\n",
    "        return x\n",
    "\n",
    "    def input_init(self, batch_size):\n",
    "        return torch.zeros(batch_size, deterministic_size).to(device)\n",
    "\n",
    "\"\"\"\n",
    "[TransitionModel]\n",
    "input  : deterministic (B, 200)\n",
    "output : Normal dist, prior (B, 30)\n",
    "\"\"\"\n",
    "class TransitionModel(nn.Module):\n",
    "    def __init__(self, min_std=0.1, hidden_size=200):\n",
    "        super().__init__()\n",
    "        self.min_std = min_std\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(deterministic_size, self.hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_size, stochastic_size * 2),\n",
    "        )\n",
    "        self.network.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, deterministic):\n",
    "        x = self.network(deterministic)\n",
    "        prior_dist = create_normal_dist(x, min_std=self.min_std)\n",
    "        prior = prior_dist.rsample()\n",
    "        return prior_dist, prior\n",
    "\n",
    "    def input_init(self, batch_size):\n",
    "        return torch.zeros(batch_size, stochastic_size).to(device)\n",
    "\n",
    "\"\"\"\n",
    "[RepresentationModel]\n",
    "input  : embedded_observation (B, 1024), deterministic (B, 200)\n",
    "output : Normal dist, posterior (B, 30)\n",
    "\"\"\"\n",
    "class RepresentationModel(nn.Module):\n",
    "    def __init__(self, min_std=0.1, hidden_size=200):\n",
    "        super().__init__()\n",
    "        self.min_std = min_std\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embedded_state_size + deterministic_size, self.hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_size, stochastic_size * 2),\n",
    "        )\n",
    "        self.network.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, embedded_observation, deterministic):\n",
    "        x = self.network(torch.cat((embedded_observation, deterministic), 1))\n",
    "        posterior_dist = create_normal_dist(x, min_std=self.min_std)\n",
    "        posterior = posterior_dist.rsample()\n",
    "        return posterior_dist, posterior\n",
    "\n",
    "\"\"\"\n",
    "[RewardPredictor]\n",
    "input  : posterior (B, T, 30), deterministic (B, T, 200)\n",
    "output : Normal dist (B, T, 1)\n",
    "\"\"\"\n",
    "class RewardPredictor(nn.Module):\n",
    "    def __init__(self, hidden_size=400):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(stochastic_size + deterministic_size, self.hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_size, 1)\n",
    "        )\n",
    "        self.network.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, posterior, deterministic):\n",
    "        x = temporal_forward(self.network, posterior, deterministic, output_shape=(1,))\n",
    "        dist = create_normal_dist(x, std=1, event_shape=1)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b2958bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Actor]\n",
    "input  : posterior (B, 30), deterministic (B, 200)\n",
    "output : action (B, 2)\n",
    "\"\"\"\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, hidden_size=400):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(stochastic_size + deterministic_size, self.hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_size, action_size)\n",
    "        )\n",
    "        self.network.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, posterior, deterministic):\n",
    "        x = torch.cat((posterior, deterministic), -1)\n",
    "        x = self.network(x)\n",
    "        dist = OneHotCategorical(logits=x)\n",
    "        action = dist.sample() + dist.probs - dist.probs.detach()\n",
    "        return action\n",
    "\n",
    "\"\"\"\n",
    "[Critic]\n",
    "input  : posterior (B, 30), deterministic (B, 200)\n",
    "output : value (B, 1)\n",
    "\"\"\"\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, hidden_size=400):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(stochastic_size + deterministic_size, self.hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_size, 1)\n",
    "        )\n",
    "        self.network.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, posterior, deterministic):\n",
    "        x = temporal_forward(self.network, posterior, deterministic, output_shape=(1,))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e21b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(module):\n",
    "    if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        kaiming_uniform_(module.weight, nonlinearity=\"relu\")\n",
    "        constant_(module.bias, 0)\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        kaiming_uniform_(module.weight)\n",
    "        constant_(module.bias, 0)\n",
    "\n",
    "def temporal_forward(network, x, y, output_shape):\n",
    "    x = torch.cat((x, y), -1) # (B, C1), (B, C2) -> (B, C1 + C2)\n",
    "\n",
    "    if x.ndim == 2:\n",
    "        B, C = x.shape\n",
    "        x = network(x)\n",
    "        x = x.reshape(B, *output_shape)\n",
    "    elif x.ndim == 3:\n",
    "        B, T, C = x.shape\n",
    "        x = x.reshape(B*T, C)\n",
    "        x = network(x)\n",
    "        x = x.reshape(B, T, *output_shape)\n",
    "    else:\n",
    "        raise ValueError(f\"input dimension must be 2 or 3 | current x.shape: {x.shape}\")\n",
    "    return x\n",
    "\n",
    "def create_normal_dist(x, std=None, min_std=0.1, event_shape=None):\n",
    "    if std is None:\n",
    "        mean, std = torch.chunk(x, 2, -1)\n",
    "        std = F.softplus(std) + min_std\n",
    "    else:\n",
    "        mean = x\n",
    "    dist = Normal(mean, std)\n",
    "    if event_shape:\n",
    "        dist = Independent(dist, event_shape)\n",
    "    return dist\n",
    "\n",
    "def compute_lambda_returns(rewards, values):\n",
    "    current_rewards = rewards[:, :-1] # r_{t}  : t (0 ~ h-1)\n",
    "    next_values     = values[:, :-1]\n",
    "\n",
    "    lambda_return = next_values[:, -1] # v_{h}  : 마지막 시점의 가치\n",
    "    td_targets = current_rewards + gamma * (1 - lambda_) * next_values # 재귀적이지 않은 부분 미리 구하기\n",
    "\n",
    "    lambda_returns = []\n",
    "    for index in reversed(range(horizon_length - 1)):\n",
    "        last = td_targets[:, index] + gamma * lambda_ * lambda_return\n",
    "        lambda_returns.append(last)\n",
    "    returns = torch.stack(list(reversed(lambda_returns)), dim=1).to(device)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15aabe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 64\n",
    "width = 64\n",
    "frame_skip = 2\n",
    "\n",
    "world_model_lr = 0.001\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.0001\n",
    "\n",
    "episodes = 1000\n",
    "world_model_update_steps = 100\n",
    "collect_episodes = 1\n",
    "\n",
    "batch_size = 50\n",
    "batch_length = 50\n",
    "\n",
    "capacity = 100000\n",
    "deterministic_size = 200\n",
    "stochastic_size = 30\n",
    "embedded_state_size = 1024\n",
    "\n",
    "free_nats = 3\n",
    "horizon_length = 15\n",
    "\n",
    "clip_grad = 100\n",
    "grad_norm_type = 2\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68d82fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\") # state : (400, 600, 3)\n",
    "env = ResizeObservationWrapper(env, height, width)     # state : (64, 64, 3)\n",
    "env = ChannelFirstEnv(env)                             # state : (3, 64, 64)\n",
    "env = FrameSkipWrapper(env, skip=frame_skip)\n",
    "env = PixelNormalizationWrapper(env)                   # [0 ~ 255] -> [-0.5 ~ 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04b1c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_shape = env.observation_space.shape # (3, 64, 64)\n",
    "action_size = env.action_space.n                # 2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = ''\n",
    "# model_path = 'model_ep200.pth' # if not -> \"\"\n",
    "\n",
    "agent = Dreamer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9966b2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting experiences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected experiences : 170\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mDreamer.train\u001b[39m\u001b[34m(self, env)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(world_model_update_steps):\n\u001b[32m     58\u001b[39m     experiences = \u001b[38;5;28mself\u001b[39m.memory.sample(batch_size, batch_length)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     posteriors, deterministics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdynamic_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.behavior_learning(posteriors, deterministics)\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m.collect_episodes(env, collect_episodes)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 162\u001b[39m, in \u001b[36mDreamer.dynamic_learning\u001b[39m\u001b[34m(self, experiences)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# size: (50, 49, 200)\u001b[39;00m\n\u001b[32m    160\u001b[39m deterministics = torch.stack(deterministics, dim=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworld_model_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprior_means\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprior_means\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprior_stds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprior_stds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposteriors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposteriors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposterior_means\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposterior_means\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposterior_stds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposterior_stds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeterministics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m posteriors.detach(), deterministics.detach()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 187\u001b[39m, in \u001b[36mDreamer.world_model_update\u001b[39m\u001b[34m(self, experiences, prior_means, prior_stds, posteriors, posterior_means, posterior_stds, deterministics)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# reconstruction loss\u001b[39;00m\n\u001b[32m    186\u001b[39m reconstruction_dist = \u001b[38;5;28mself\u001b[39m.decoder(posteriors, deterministics)\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m reconstruction_loss = \u001b[43mreconstruction_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# reward loss\u001b[39;00m\n\u001b[32m    190\u001b[39m reward_dist = \u001b[38;5;28mself\u001b[39m.reward_predictor(posteriors, deterministics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/distributions/independent.py:108\u001b[39m, in \u001b[36mIndependent.log_prob\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     log_prob = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _sum_rightmost(log_prob, \u001b[38;5;28mself\u001b[39m.reinterpreted_batch_ndims)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/distributions/normal.py:84\u001b[39m, in \u001b[36mNormal.log_prob\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# compute the variance\u001b[39;00m\n\u001b[32m     82\u001b[39m var = \u001b[38;5;28mself\u001b[39m.scale**\u001b[32m2\u001b[39m\n\u001b[32m     83\u001b[39m log_scale = (\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     math.log(\u001b[38;5;28mself\u001b[39m.scale) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.scale, Real) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m )\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     87\u001b[39m     -((value - \u001b[38;5;28mself\u001b[39m.loc) ** \u001b[32m2\u001b[39m) / (\u001b[32m2\u001b[39m * var)\n\u001b[32m     88\u001b[39m     - log_scale\n\u001b[32m     89\u001b[39m     - math.log(math.sqrt(\u001b[32m2\u001b[39m * math.pi))\n\u001b[32m     90\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "agent.train(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
